# Model Configuration
base_model: meta-llama/Llama-3.2-3B-Instruct
model_type: AutoModelForCausalLM
tokenizer_type: AutoTokenizer
trust_remote_code: true

# Dataset Configuration
datasets:
  - path: training_data_llama_bge_filtered.jsonl
    type: chat_template
    field_messages: messages
val_set_size: 0.15

# Training Parameters
sequence_len: 1024
micro_batch_size: 16
gradient_accumulation_steps: 5
num_epochs: 15

# LoRA Adapter Configuration
adapter: lora
lora_r: 64
lora_alpha: 128
lora_dropout: 0.15
lora_target_modules:
  - q_proj
  - k_proj
  - v_proj
  - o_proj
lora_target_linear: false
lora_fan_in_fan_out: false

# Optimizer Configuration
optimizer: adamw_torch_fused
learning_rate: 2e-5
lr_scheduler: cosine
lr_warmup_steps: 300
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.999
adam_epsilon: 1e-8

# Save and Evaluation Configuration
output_dir: outputs/training_data_llama_bge_filtered
save_strategy: steps
save_steps: 200
eval_strategy: steps
eval_steps: 100
metric_for_best_model: eval_loss
greater_is_better: false
load_best_model_at_end: true
logging_steps: 25
save_total_limit: 5
report_to: wandb
wandb_project: training_data_llama_bge_filtered
wandb_run_name: "training_data_llama_bge_filtered"

# Early Stopping Configuration
early_stopping_patience: 3
early_stopping_threshold: 0.001

# Memory Optimization
bf16: true
tf32: true
flash_attention: true
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: false
group_by_length: true
dataloader_num_workers: 4
dataloader_pin_memory: true
max_grad_norm: 0.5

# Stability Enhancement
warmup_ratio: 0.2
label_smoothing_factor: 0.05
dropout: 0.05
fp16_full_eval: false

# Data Optimization
data_seed: 42
shuffle: true
dataloader_drop_last: false
remove_unused_columns: true
dataloader_prefetch_factor: 2
dataloader_persistent_workers: true

# Special Tokens
special_tokens:
  bos_token: "<|begin_of_text|>"
  eos_token: "<|eot_id|>"
  pad_token: "<|eot_id|>"

# Additional Stability Options
save_safetensors: true
torch_compile: false
logging_first_step: true
logging_nan_inf_filter: true
ddp_find_unused_parameters: false
fsdp: []

# NaN Prevention Settings
ignore_data_skip: true
skip_memory_metrics: false
dataloader_drop_last: true
eval_accumulation_steps: 4